{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5494604",
   "metadata": {},
   "source": [
    "## SENTIMENT ANALYSIS WITH AMAZON REVIEWS ##"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcc3a038",
   "metadata": {},
   "source": [
    "In this analysis we are going to analyze the several millions of reviews of the different amazon products.\n",
    "(And also in the text file , there is complete overview of the text data).\n",
    "\n",
    "Here we have text data so this will be an interesting work for doing the NLP work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec125e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries (python 3 version provided many of this but if there is missing any library you can directly \n",
    "# download and install in this terminal with command  '!pip insatll packagename')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras import models, layers, optimizers\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import bz2\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a57c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have to read the given reviews text files :\n",
    "# The text is held in a compressed format. Luckily, we can still read it line by line. The first word gives the label, \n",
    "# so we have to convert that into a number and then take the rest to be the comment.\n",
    "\n",
    "def get_labels_and_texts(file):\n",
    "    labels = []\n",
    "    texts = []\n",
    "    for line in bz2.BZ2File(file):\n",
    "        x = line.decode(\"utf-8\")\n",
    "        labels.append(int(x[9]) - 1)\n",
    "        texts.append(x[10:].strip())\n",
    "    return np.array(labels), texts\n",
    "train_labels, train_texts = get_labels_and_texts('train.ft.txt.bz2')\n",
    "test_labels, test_texts = get_labels_and_texts('test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ba2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing :\n",
    "# The first thing I'm going to do to process the text is to lowercase everything and then remove non-word characters. \n",
    "# I replace these with spaces since most are going to be punctuation. Then I'm going to just remove any other \n",
    "# characters (like letters with accents). It could be better to replace some of these with regular ascii characters \n",
    "#but I'm just going to ignore that here. It also turns out if you look at the counts of the different characters that there \n",
    "# are very few unusual characters in this corpus.\n",
    "\n",
    "import re\n",
    "NON_ALPHANUM = re.compile(r'[\\W]')\n",
    "NON_ASCII = re.compile(r'[^a-z0-1\\s]')\n",
    "def normalize_texts(texts):\n",
    "    normalized_texts = []\n",
    "    for text in texts:\n",
    "        lower = text.lower()\n",
    "        no_punctuation = NON_ALPHANUM.sub(r' ', lower)\n",
    "        no_non_ascii = NON_ASCII.sub(r'', no_punctuation)\n",
    "        normalized_texts.append(no_non_ascii)\n",
    "    return normalized_texts\n",
    "        \n",
    "train_texts = normalize_texts(train_texts)\n",
    "test_texts = normalize_texts(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd233903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting up the test and train data :\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, random_state=57643892, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f7bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras provides some tools for converting text to formats that are useful in deep learning models. \n",
    "# I've already done some processing, so now I will just run a Tokenizer using the top 12000 words as features.\n",
    "MAX_FEATURES = 12000\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "train_texts = tokenizer.texts_to_sequences(train_texts)\n",
    "val_texts = tokenizer.texts_to_sequences(val_texts)\n",
    "test_texts = tokenizer.texts_to_sequences(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2aced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding Sequences :\n",
    "# In order to use batches effectively, I'm going to need to take my sequences and turn them into sequences of the same length. \n",
    "# I'm just going to make everything here the length of the longest sentence in the training set. I'm not dealing with this here,\n",
    "# but it may be advantageous to have variable lengths so that each batch contains sentences of similar lengths. This might help \n",
    "# mitigate issues that arise from having too many padded elements in a sequence. There are also different padding modes that \n",
    "# might be useful for different models.\n",
    "\n",
    "MAX_LENGTH = max(len(train_ex) for train_ex in train_texts)\n",
    "train_texts = pad_sequences(train_texts, maxlen=MAX_LENGTH)\n",
    "val_texts = pad_sequences(val_texts, maxlen=MAX_LENGTH)\n",
    "test_texts = pad_sequences(test_texts, maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8654b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network Model :\n",
    "# I'm just using fairly simple models here. This CNN has an embedding with a dimension of 64, 3 convolutional layers with the \n",
    "# first two having match normalization and max pooling and the last with global max pooling. The results are then passed to a \n",
    "# dense layer and then the output.\n",
    "\n",
    "def build_model():\n",
    "    sequences = layers.Input(shape=(MAX_LENGTH,))\n",
    "    embedded = layers.Embedding(MAX_FEATURES, 64)(sequences)\n",
    "    x = layers.Conv1D(64, 3, activation='relu')(embedded)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPool1D(3)(x)\n",
    "    x = layers.Conv1D(64, 5, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPool1D(5)(x)\n",
    "    x = layers.Conv1D(64, 5, activation='relu')(x)\n",
    "    x = layers.GlobalMaxPool1D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(100, activation='relu')(x)\n",
    "    predictions = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs=sequences, outputs=predictions)\n",
    "    model.compile(\n",
    "        optimizer='rmsprop',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_accuracy']\n",
    "    )\n",
    "    return model\n",
    "    \n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d01630",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_texts, \n",
    "    train_labels, \n",
    "    batch_size=128,\n",
    "    epochs=2,\n",
    "    validation_data=(val_texts, val_labels), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b8d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_texts)\n",
    "print('Accuracy score: {:0.4}'.format(accuracy_score(test_labels, 1 * (preds > 0.5))))\n",
    "print('F1 score: {:0.4}'.format(f1_score(test_labels, 1 * (preds > 0.5))))\n",
    "print('ROC AUC score: {:0.4}'.format(roc_auc_score(test_labels, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b0aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d43eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Network Model :\n",
    "# For an RNN model I'm also going to use a simple model. This has an embedding, two GRU layers, followed by 2 dense layers and \n",
    "# then the output layer. I'm using the CuDNNGRU rather than GRU because the former will run much faster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a59daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.fit(\n",
    "    train_texts, \n",
    "    train_labels, \n",
    "    batch_size=128,\n",
    "    epochs=1,\n",
    "    validation_data=(val_texts, val_labels), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2ebfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = rnn_model.predict(test_texts)\n",
    "print('Accuracy score: {:0.4}'.format(accuracy_score(test_labels, 1 * (preds > 0.5))))\n",
    "print('F1 score: {:0.4}'.format(f1_score(test_labels, 1 * (preds > 0.5))))\n",
    "print('ROC AUC score: {:0.4}'.format(roc_auc_score(test_labels, preds)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
